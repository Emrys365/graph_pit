{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runtime evaluation similar to [1]\n",
    "References:\n",
    "[1] Speeding Up Permutation Invariant Training for Source Separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "import paderbox as pb\n",
    "import itertools\n",
    "import timeit\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import numpy as np\n",
    "import padertorch as pt\n",
    "import paderbox as pb\n",
    "from tqdm.notebook import tqdm\n",
    "import graph_pit\n",
    "import torch\n",
    "import padertorch as pt\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General settings\n",
    "max_time = 1  # Time in s over which runs are ignored\n",
    "device = 'cpu' # Device for computing the losses / loss matrices. The permutation solver always works on the CPU\n",
    "number = 3  # Number of runs per configuration. Higher number means smoother curves, but large values are impractical for an interactive notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilty functions\n",
    "\n",
    "def plot_timings(timings, xrange, xlabel, logx=False):\n",
    "    with pb.visualization.axes_context() as ac:\n",
    "        for idx, (key, values) in enumerate(timings.items()):\n",
    "            values = np.asarray(values)\n",
    "            x = xrange[:len(values)]\n",
    "            pb.visualization.plot.line(x, values.mean(axis=-1), label=key, ax=ac.last, color=f'C{idx}')\n",
    "            ac.last.fill_between(x, values.min(axis=-1), values.max(axis=-1), color=f'C{idx}', alpha=0.3)\n",
    "    #         std = values.std(axis=-1)\n",
    "    #         mean = values.mean(axis=-1)\n",
    "    #         ac.last.fill_between(x, mean - std, mean + std, color=f'C{idx}', alpha=0.3)\n",
    "        if logx:\n",
    "            ac.last.loglog()\n",
    "        else:\n",
    "            ac.last.semilogy()\n",
    "        ac.last.set_xlabel(xlabel)\n",
    "        ac.last.set_ylabel('runtime in s')\n",
    "        ac.last.set_ylim([ac.last.get_ylim()[0], max_time])\n",
    "        ac.last.set_xlim([xrange[0], xrange[-1]]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## uPIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from padertorch.ops.losses.source_separation import pit_loss_from_loss_matrix, compute_pairwise_losses\n",
    "from torch.nn.functional import mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the uPIT loss functions\n",
    "\n",
    "def upit_sa_sdr_decomp_dot(estimate, target, algorithm='hungarian'):\n",
    "    \"\"\"\n",
    "    sa-SDR decomposed with dot product, eq. (13)/(14)\n",
    "    \"\"\"\n",
    "    loss_matrix = -torch.matmul(estimate, target.T)\n",
    "    loss = pit_loss_from_loss_matrix(\n",
    "        loss_matrix, reduction='sum', algorithm=algorithm\n",
    "    )\n",
    "    numerator = torch.sum(target**2)\n",
    "    loss = -10*(torch.log10(numerator) - torch.log10(\n",
    "        numerator + torch.sum(estimate**2) + 2*loss\n",
    "    ))\n",
    "    return loss\n",
    "\n",
    "def upit_sa_sdr_decomp_mse(estimate, target, algorithm='hungarian'):\n",
    "    \"\"\"\n",
    "    sa-SDR decomposed with MSE, eq. (11)/(12)\n",
    "    \"\"\"\n",
    "    loss_matrix = compute_pairwise_losses(estimate, target, axis=0, loss_fn=functools.partial(mse_loss, reduction='sum'))\n",
    "    loss = pit_loss_from_loss_matrix(\n",
    "        loss_matrix, reduction='sum', algorithm=algorithm\n",
    "    )\n",
    "    loss = -10*(torch.log10(torch.sum(target**2)) - torch.log10(\n",
    "        loss\n",
    "    ))\n",
    "    return loss\n",
    "\n",
    "def upit_sa_sdr_naive_brute_force(estimate, target):\n",
    "    \"\"\"\n",
    "    Brute-force sa-SDR, eq. (5)\n",
    "    \"\"\"\n",
    "    return pt.pit_loss(estimate, target, 0, pt.source_aggregated_sdr_loss)\n",
    "\n",
    "def upit_a_sdr_naive_brute_force(estimate, target):\n",
    "    \"\"\"\n",
    "    Brute-force a-SDR\n",
    "    \"\"\"\n",
    "    return pt.pit_loss(estimate, target, 0, pt.sdr_loss)\n",
    "\n",
    "def upit_a_sdr_decomp(estimate, target, algorithm='hungarian'):\n",
    "    \"\"\"\n",
    "    Decomposed a-SDR\n",
    "    \"\"\"\n",
    "    loss_matrix = compute_pairwise_losses(estimate, target, axis=0, loss_fn=pt.sdr_loss)\n",
    "    loss = pit_loss_from_loss_matrix(\n",
    "        loss_matrix, reduction='mean', algorithm=algorithm\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the losses all give the same loss values\n",
    "estimate = torch.randn(3, 32000)\n",
    "target = torch.randn(3, 32000)\n",
    "\n",
    "ref = upit_sa_sdr_naive_brute_force(estimate, target)\n",
    "np.testing.assert_allclose(ref, upit_sa_sdr_decomp_dot(estimate, target), rtol=1e-5)\n",
    "np.testing.assert_allclose(ref, upit_sa_sdr_decomp_dot(estimate, target, algorithm='brute_force'), rtol=1e-5)\n",
    "np.testing.assert_allclose(ref, upit_sa_sdr_decomp_mse(estimate, target), rtol=1e-5)\n",
    "np.testing.assert_allclose(ref, upit_sa_sdr_decomp_mse(estimate, target, algorithm='brute_force'), rtol=1e-5)\n",
    "\n",
    "ref = upit_a_sdr_naive_brute_force(estimate, target)\n",
    "np.testing.assert_allclose(ref, upit_a_sdr_decomp(estimate, target), rtol=1e-5)\n",
    "np.testing.assert_allclose(ref, upit_a_sdr_decomp(estimate, target, algorithm='brute_force'), rtol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all loss functions whose runtime we want to compare\n",
    "losses = {\n",
    "    'sa_sdr naive brute_force': upit_sa_sdr_naive_brute_force,\n",
    "    'sa_sdr brute_force deomp mse': functools.partial(upit_sa_sdr_decomp_mse, algorithm='brute_force'),\n",
    "    'sa_sdr brute_force deomp dot': functools.partial(upit_sa_sdr_decomp_dot, algorithm='brute_force'),\n",
    "    'sa_sdr hungarian decomp mse': upit_sa_sdr_decomp_mse,\n",
    "    'sa_sdr hungarian decomp dot': upit_sa_sdr_decomp_dot,\n",
    "    'a_sdr naive brute_force': upit_a_sdr_naive_brute_force,\n",
    "    'a_sdr decomp brute_force': functools.partial(upit_a_sdr_decomp, algorithm='brute_force'),\n",
    "    'a_sdr decomp hungarian': upit_a_sdr_decomp,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for uPIT\n",
    "num_speakers_range = list(range(2, 100))\n",
    "T = 32000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_loss(loss, num_speakers=3, T=8000 * 4, number=10, device='cuda'):\n",
    "    import torch\n",
    "    targets = torch.tensor(np.random.randn(num_speakers, T)).to(device)\n",
    "    estimates = torch.tensor(np.random.randn(num_speakers, T)).to(device)\n",
    "    timings = timeit.repeat('float(loss(estimates, targets).cpu())', globals=locals(), repeat=number, number=1)\n",
    "    timings = np.asarray(timings)\n",
    "    return timings\n",
    "\n",
    "upit_timings = defaultdict(list)\n",
    "skip = defaultdict(lambda: False)\n",
    "\n",
    "for num_speakers in tqdm(num_speakers_range):\n",
    "    for loss_name, loss_fn in losses.items():\n",
    "        if skip[loss_name]:\n",
    "            continue\n",
    "        timing = time_loss(loss_fn, num_speakers=num_speakers, number=number, device=device, T=T)\n",
    "        upit_timings[loss_name].append(timing)\n",
    "        if np.mean(timing) > max_time:\n",
    "            skip[loss_name] = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_timings(upit_timings, num_speakers_range, '#speakers', logx=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Brute-force becomes impractical for very small numbers of speakers (<10)\n",
    "- The Hungarian Algorithm can be used for large numbers of speakers with no significant runtime\n",
    "- The dot decomposition is the fastest here. It is, however, probably possible to push the MSE below the dot with a low-level implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph-PIT assignment algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_pit_losses = {\n",
    "    'naive brute-force': graph_pit.loss.unoptimized.GraphPITLossModule(pt.source_aggregated_sdr_loss),\n",
    "    'decomp brute-force': graph_pit.loss.optimized.OptimizedGraphPITSourceAggregatedSDRLossModule(assignment_solver='optimal_brute_force'),\n",
    "    'decomp branch-and-bound': graph_pit.loss.optimized.OptimizedGraphPITSourceAggregatedSDRLossModule(assignment_solver='optimal_branch_and_bound'),\n",
    "    'decomp dfs': graph_pit.loss.optimized.OptimizedGraphPITSourceAggregatedSDRLossModule(assignment_solver='dfs'),\n",
    "    'decomp dynamic programming': graph_pit.loss.optimized.OptimizedGraphPITSourceAggregatedSDRLossModule(assignment_solver='optimal_dynamic_programming'),\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_utterances_range = list(range(2, 30))\n",
    "utterance_length = 8000\n",
    "overlap = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_alg(loss, num_segments, num_estimates=3, number=10, device='cpu',\n",
    "             utterance_length=2*8000, overlap=500):\n",
    "    timings = []\n",
    "    for i in range(number):\n",
    "        segment_boundaries = [\n",
    "            (i * (utterance_length - overlap), (i + 1) * utterance_length)\n",
    "            for i in range(num_segments)\n",
    "        ]\n",
    "        num_samples = max(s[-1] for s in segment_boundaries) + 100\n",
    "        targets = [torch.rand(stop - start).to(device) for start, stop in segment_boundaries]\n",
    "        estimate = torch.rand(num_estimates, num_samples).to(device)\n",
    "\n",
    "        timings.append(timeit.timeit(\n",
    "            # 'float(l.loss.cpu().numpy())',\n",
    "            setup='l = loss.get_loss_object(estimate, targets, segment_boundaries)',\n",
    "            stmt='float(l.loss.cpu())',\n",
    "            globals={\n",
    "            'loss': loss,\n",
    "            'estimate': estimate,\n",
    "            'targets': targets,\n",
    "            'segment_boundaries': segment_boundaries,\n",
    "        }, number=1))\n",
    "    return np.asarray(timings)\n",
    "\n",
    "graph_pit_timings = defaultdict(list)\n",
    "skip = defaultdict(lambda: False)\n",
    "\n",
    "for num_segments in tqdm(num_utterances_range):\n",
    "    for loss_name, loss_fn in graph_pit_losses.items():\n",
    "        if skip[loss_name]:\n",
    "            continue\n",
    "        timing = time_alg(loss_fn, num_segments=num_segments, number=number, device='cpu', utterance_length=utterance_length, overlap=overlap)\n",
    "        graph_pit_timings[loss_name].append(timing)\n",
    "        if np.mean(timing) > max_time:\n",
    "            skip[loss_name] = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_timings(graph_pit_timings, num_utterances_range, '#utterances')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The brute-force variants quickly become inpractical for training a network\n",
    "- The branch-and-bound algorithm has a much larger variance in its runtime than all other algorithms\n",
    "- The dynamic programming algorithm has a similar runtime compared to the DFS algorithm, but it always finds the optimal coloring\n",
    "- DFS and dynamic programming have a runtime that is neglectible compared to common network architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
